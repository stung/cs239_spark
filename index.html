<!DOCTYPE html>
<html>
<head>
<!--<script src="jquery-1.11.2.min.js"></script><script src="headingClicks.js"></script>-->
<title>Spark Tutorial</title>
</head>

<body>
<h1><img src="spark-logo.png" alt="Spark Logo" style="width:150px;height:100px"> Tutorial</h1>
<p>Demo demo demo things</p>

<h2>Introduction</h2>
<p>
	Spark is a relatively new, general-purpose cluster computing software system. It has APIs in Java, Scala, 
	and Python, allowing for a wide range of options in terms of programming languages.
</p>
<p>
	A Spark application utilizes a <b>driver program</b> that runs the main function. The driver also
	coordinates the parallel efforts of the child programs on the cluster. Each individual node on the cluster
	consists of multiple <b>resilient distributed datasets</b> (RDDs) that are the foundation of Spark (which 
	we will discuss in greater detail). Simply put, RDDs enable users to parallelize their driver program, or
	run operations on an external dataset (of which anything using a Hadoop InputFormat is supported). The user
	can also specify that an RDD persist in memory, which greatly speeds up its parallel processing (assuming
	the memory is large enough to accommodate the RDD). They are also very fault tolerant, which is a key 
	feature for any distributed computing system (as we have already established with MapReduce).
</p>
<p>
	Spark also uses <b>shared variables</b> that can be accessed from the different nodes. Normally, local 
	variables in a parallelized function are copied over onto the different nodes. On certain occasions, 
	special shared variables may greatly optimize a parallel operation. These come in two flavors:
	<ol>
		<li>
			<b>Broadcast Variables:</b> These are variables that are cached in memory on each node that uses it.
			As they are read-only, there is no need to worry about data mismatchs between the different copies
			over a period of time.
		</li>
		<li>
			<b>Accumulators:</b> These are variables that will only ever increase in value. This makes it simple 
			to write to from the cluster nodes, as they never need to confirm the original value before writing 
			to it. Only the driver program is able to read the accumulated value.
		</li>
	</ol>
</p>

<h2>Setup</h2>
<p>
	Simply go to the <a href="http://spark.apache.org/downloads.html"> Spark Page</a>, where you will
	find many different options for downloading the system.
</p>
<p>
	We ultimately decided to pick a pre-built version of Spark, though the source code is also readily 
	available for a user to build on their own.
</p>
<p>
	Troubleshooting: Spark may not run if
</p>

<h2>RDD</h2>
<p>
	As mentioned before, Spark is implemented on top of Resilient Distributed Datasets.
</p>

<h2>MapReduce</h2>

<h2>WordCount</h2>

<h2>PiEstimator</h2>

<h2>PageRank</h2>

<h2>SparkSQL</h2>

<h2>Spark vs Hadoop</h2>
<p>Hadoop is Hadoopy.</p>
<p>More comparisons from the Spark paper</p>


</body>
</html>
