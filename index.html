<!DOCTYPE html>
<html>
<head>
<!--<script src="jquery-1.11.2.min.js"></script><script src="headingClicks.js"></script>-->
	<title>Spark Tutorial</title>
	<link rel="stylesheet" href="style.css">
</head>

<body>

<div id="bodyStyle">
	<h1><img src="spark-logo.png" alt="Spark Logo" style="width:150px;height:100px"> Tutorial</h1>
	<p>Demo demo demo things</p>

	<h2 class="header">Introduction</h2>

	<div class="section">
		<p>
			Spark is a relatively new, general-purpose cluster computing software system. It has APIs in Java, Scala, 
			and Python, allowing for a wide range of options in terms of programming languages.
		</p>
		<p>
			A Spark application utilizes a <b>driver program</b> that runs the main function. The driver also
			coordinates the parallel efforts of the child programs on the cluster. Each individual node on the cluster
			consists of multiple <b>resilient distributed datasets</b> (RDDs) that are the foundation of Spark (which 
			we will discuss in greater detail). Simply put, RDDs enable users to parallelize their driver program, or
			run operations on an external dataset (of which anything using a Hadoop InputFormat is supported). The user
			can also specify that an RDD persist in memory, which greatly speeds up its parallel processing (assuming
			the memory is large enough to accommodate the RDD). They are also very fault tolerant, which is a key 
			feature for any distributed computing system (as we have already established with MapReduce).
		</p>
		<p>
			Spark also uses <b>shared variables</b> that can be accessed from the different nodes. Normally, local 
			variables in a parallelized function are copied over onto the different nodes. On certain occasions, 
			special shared variables may greatly optimize a parallel operation. These come in two flavors:
			<ol>
				<li>
					<b>Broadcast Variables:</b> These are variables that are cached in memory on each node that uses it.
					As they are read-only, there is no need to worry about data mismatchs between the different copies
					over a period of time. This will also ensure that new nodes will receive the same value if more nodes
					are required for the driver.
				</li>
				<li>
					<b>Accumulators:</b> These are variables that will only ever increase in value. This makes it simple 
					to write to from the cluster nodes, as they never need to confirm the original value before writing 
					to it. Only the driver program is able to read the accumulated value.
				</li>
			</ol>
		</p>
		<p>
			Deployment to a cluster is typically accomplished by packaging the program into a JAR file (when using Java 
			or Scala), then submitting it using the <code>./bin/spark-submit</code> script.
		</p>
	</div>

	<h2 class="header">Setup</h2>

	<div class="section">
		<p>
There are two basic ways to set up a Spark environment: pre-built or source code.
</p>
<p>
<b>Download</b><br />
To download Spark, visit <a href="http://spark.apache.org/downloads.html">http://spark.apache.org/downloads.html</a>
</p>
<p>
<b>Pre-Built</b><br />
Select a pre-built package from the download section, and download.<br />
</p>
<p>
<b>Source Code</b><br />
After downloading the source code, run<br />
<pre><code>export MAVEN_OPTS="-Xmx2g -XX:MaxPermSize=512M -XX:ReservedCodeCacheSize=512m"
</code></pre>
This will increase the max maven memory to allow building the source.<br /><br />
Then run the build by executing the code<br />
<pre><code>mvn -DskipTests clean package
</code></pre>
This will take a while to complete.
</p>
<p>
<b>Test Environment</b><br />
After the build is successful, you may test your environment by running<br />
<pre><code>./bin/run-example SparkPi 10
</code></pre>
</p>
<p>
<b>Keep in mind</b><br />
Spark build is best when using scala 2.10.X<br />
The latest scala will cause complications and may not build.<br />
Also, building Spark requires Maven and Java.
</p>
	</div>

	<h2 class="header">Resilient Distributed Datasets (RDDs)</h2>

	<div class="section">
		<p>
			As mentioned before, Spark is implemented on top of RDDs. These RDDs are key to understanding how Spark is able
			to claim more efficient memory management while still performing as well as the other big distributed systems.
			The basic idea is that RDDs keep track of coarse-grained transformations that have been applied to it, rather
			than fine-grained operations to its shared state. That is to say, a fine-grained operation would involve updating
			specific cells in a shared table, which would involve propagating this change across all other nodes that use it
			so that it can be recovered in the event of failure. A coarse-grained approach involves using one operation to
			update most, if not all, elements. This allows the driver to simply keep track of all transformations utilized
			over time, allowing an RDD to easily rebuild from failure by checking other RDDs. In spite of this potential 
			loss of generality, RDDs are still very powerful and flexible enough to handle the majority of parallelizable 
			computations.
		</p>
		<p>
			Formally speaking, an RDD is actually immutable - any modifications to an RDD are actually a result of a
			transformation creating a new RDD after running a function through a group of elements in the old one. In fact,
			RDDs may not even need to stay in memory. It keeps track of how it was derived from other data through the
			aforementioned transformations, known as its <b>lineage</b>, and is able to compute specific partitions of itself
			on the fly. The user can also specify which RDDs may be queried more often, and therefore should be stored in
			memory instead of being represented as a series of transformations. This is known as <b>persistence</b>, which
			greatly speeds up execution if a user knows beforehand that a certain dataset will be referenced multiple times.
			An RDD can also be <b>partitioned</b> across multiple machines, which is another aspect of RDDs that can be set by
			the user.
		</p>
		<h3>RDD Operations</h3>
		<p>
			There are two kinds of operations one can perform on RDDs. The first is a <b>transformation</b>, which modifies
			an existing dataset, and the second is an <b>action</b>, which will compute a value from a dataset and return 
			it to the driver program. The classic transformation and action example would be <code>map</code> and 
			<code>reduce</code>: <code>map</code> takes a function and applies it to every element in a dataset, which is how 
			a transformation works. <code>reduce</code> takes a dataset and aggregates it through a function - this is 
			analogous to how an action operates and passes its value on to the driver program.
		</p>
		<p>
			Transformations in Spark are <b>lazy</b>, which means that the dataset is not modified by the transformations
			immediately. The transformation sequence is saved and only applied when an action requests the value for the
			driver program.
		</p>
		
		<h3>RDD Advantages</h3>
		<p>
			<img src="rddVSdsm.png" alt="Table 1" style="width:450px;height:350px">
		</p>
		<p>
			Table 1 lists out a quick summary of how an RDD-based system stacks up with a generic Distributed Shared Memory 
			(DSM) system. Most importantly, it shows how the fault tolerance of RDDs can be so much more efficient than that 
			of a DSM by using lineage to track its RDDs. This prevents the necessity of periodic checkpoints and required 
			full-system rollbacks when critical nodes fail, which are typical fault recovery methods for DSMs. In addition,
			RDDs are capable of running extra copies of slow tasks, similar to how MapReduce deals with stragglers. In DSM, 
			two copies of the same task would be accessing the same memory locations, creating contention and potential data
			inconsistencies across the board.
		</p>
		<p>
			Though RDDs can be generalized to address a wide range of parallelized problems, it is also because most distributed
			systems today follow a MapReduce-type of framework, in which one function is applied over a large batch of elements.
			In the event that fine grained updates are required, RDDs would no longer be a practical choice. An example of such 
			include a storage system over multiple users for a web app, in which it would not be practical to assume that every 
			user would enjoy having the contents of their storage modified to match everyone else's.
		</p>
	</div>

	<div class="header">
		<h2>A Little About Spark Map, Reduce, and Filter</h2>
	</div>

	<div class="header">
		<h2>Spark Shell And WordCount</h2>
	</div>

	<div class="header">
		<h2>App Deployment and PiEstimator</h2>
	</div>
	<div class="section">
<h3>App Deployment</h3>
<p>
Now that we know how to use Spark with the shell, it's useful to look at a way to deploy a Spark application which can be reused. We will use Spark's "SimpleApp" to demonstrate.
</p>
<p>
<pre><code>
/* SimpleApp.scala */
import org.apache.spark.SparkContext
import org.apache.spark.SparkContext._
import org.apache.spark.SparkConf

object SimpleApp {
  def main(args: Array[String]) {
    val logFile = "README.md" // Should be some file on your system
    val conf = new SparkConf().setAppName("Simple Application")
    val sc = new SparkContext(conf)
    val logData = sc.textFile(logFile, 2).cache()
    val numAs = logData.filter(line => line.contains("a")).count()
    val numBs = logData.filter(line => line.contains("b")).count()
    println("Lines with a: %s, Lines with b: %s".format(numAs, numBs))
  }
}
</code></pre>
</p>
<p>
You can use any text file, and this application will count the number of As and the number of Bs in the file.
</p>
<p>
SparkConf is a configuration requirement to make sure that the application will be using Spark. To create the sbt configuration file, simple.sbt, use the following:
</p>
<p>
<pre><code>
name := "Simple Project"

version := "1.0"

scalaVersion := "2.10.4"

libraryDependencies += "org.apache.spark" %% "spark-core" % "1.2.0"
</code></pre>
</p>
<p>
Finally, the following steps will deploy and run the application.<br />
<pre><code>
# Your directory layout should look like this
$ find .
.
./simple.sbt
./src
./src/main
./src/main/scala
./src/main/scala/SimpleApp.scala

# Package a jar containing your application
$ sbt package
...
[info] Packaging {..}/{..}/target/scala-2.10/simple-project_2.10-1.0.jar

# Use spark-submit to run your application
$ YOUR_SPARK_HOME/bin/spark-submit \
  --class "SimpleApp" \
  --master local[4] \
  target/scala-2.10/simple-project_2.10-1.0.jar
...
Lines with a: 46, Lines with b: 23
</code></pre>
</p>

<h3>Pi Estimator</h3>
<p>
Spark comes with a sample pi estimator to demonstrate and to test your Spark environment.
</p>
<p>
Take a look at the sample code.
</p>
<pre><code>
package org.apache.spark.examples
import scala.math.random
import org.apache.spark._

object SparkPi {
def main(args: Array[String]) {
val conf = new SparkConf().setAppName("Spark Pi")
val spark = new SparkContext(conf)
val slices = if (args.length > 0) args(0).toInt else 2
val n = math.min(100000L * slices, Int.MaxValue).toInt // avoid overflow
val count = spark.parallelize(1 until n, slices).map { i =>
val x = random * 2 - 1
val y = random * 2 - 1
if (x*x + y*y < 1) 1 else 0
}.reduce(_ + _)
println("Pi is roughly " + 4.0 * count / n)
spark.stop()
}
}
</code></pre>
</p>
<p>
<i>Spark supports Python, Java, and Scala. But we will be using Scala throughout our tutorial. </i>
</p>
<p>
From this example, you can see that Spark supports standard library imports. This is also true for Java. For this Pi Estimator, the math library has been imported.
</p>
<p>
Diving into the code, the first two objects created are the configuration and the spark context. The Spark context is the object to use to perform Spark functions.
</p>
<p>
As you can see, scala allows for a Javascript style chaining of functions. And you can get a list of actions and transformations in the API or find the most frequently used actions and transformations in the programming guide <a href="http://spark.apache.org/docs/1.2.0/programming-guide.html">here</a>.
</p>
<p>
The parallelize transformation takes the given data and prepares it for parallel operation on the cluster.
</p>
<p>
Finally, the map and reduce algorithms are applied, and the result is printed.
</p>
	</div>

	<div class="header">
		<h2>PageRank</h2>
	</div>

	<div class="header">
		<h2>How Spark Stacks Up</h2>
	</div>

	<div class="section">
		<p>
			RDDs were first proposed in a paper published by UC Berkeley. There, they describe how they implements these 
			resilient datasets on top of Spark, which have now become an integral part of Spark itself. The researchers were 
			also able to show that Spark can run up to 20 times faster than Hadoop for certain iterative applications, and can 
			scan a 1 TB dataset wth only 5 - 7 seconds of latency. Fault recovery was noticeably efficient, as Spark is able to
			recover just by building the failed partitions. Specific examples are as follow:
		</p>
		<h3>Iterative Machine Learning Algorithm Evaluation</h3>
		<p>
			The authors compared their RDDs in Spark against the standard Hadoop and a Hadoop deployment known as HadoopBinMem 
			(which converts text data into binary after the initial iteration). They implemented two such machine learning 
			algorithms, one for a logistic regression and the other for k-means. Dataset sizes were 100 GB on clusters of 25 - 
			100 machines. While the k-means algorithm involves a lot of computation, the regression is more sensitive to I/O and
			other kinds of overhead. These are qualities that will be taken into consideration when evaluating the results.
		</p>
		<p>
			<img src="iterCompare.png" alt="Iteration Comparisons" style="width:450px;height:250px">
		</p>
		<p>
			This graph depicts the time difference between the initial iteration of each system and the average of subsequent 
			iterations thereafter.
		</p>
		<p>
			<img src="clusterSize.png" alt="Cluster Size Comparisons" style="width:450px;height:250px">
		</p>
		<p>
			This graph only shows the average running time of the subsequent iterations, but compares the performance of each
			one over the different numbers of machines used.
		</p>
		<h3>Fault Recovery Evaluation</h3>
		<p>
			<img src="failGraph.png" alt="Failure Graph" style="width:400px;height:200px">
		</p>
		<p>
			To test how Spark performed in the event of failure, the authors introduced a failure in the 6th iteration when 
			running the k-means algorithm. The graph shows that while Spark did have to compensate for the sudden failure,
			it was able to recover within a meaningful amount of time and continue the successive iterations with no problem.
			This is because the other machines were able to reconstruct the downed RDD through its lineage, and did not have
			to resort to rolling back to a previous state and restarting several iterations (as would typically be the case 
			for a checkpoint-based fault tolerance mechanism, depending on how often checkpoints were made).
		</p>

		<p>
			The paper and all the relevant figures used in this tutorial can be found 
			<a href="https://www.cs.berkeley.edu/~matei/papers/2012/nsdi_spark.pdf">here</a>. Further examples and implementation
			details are also found in that paper.
		</p>
	</div>

</div>
</body>
</html>
